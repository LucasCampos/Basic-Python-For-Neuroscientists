{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to write files in python. The default way, while general, is not the most suitable for most scientific applications. In science, we tend to have very structured data. We will take advantage of that to simplify our IO (input/output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The standard way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, first we will open a _file handler_, then write some stuff inside and finally close it. We will not use it again for ther rest of this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open a file called \"test.dat\", where we will write some data\n",
    "f = open(\"03_standard_way.dat\", \"w\") #here \"w\" means we are going to *write* the file. You can replace it by \"r\" to read, and \"r+\" for appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0, 1) #Generate some data \n",
    "for x in a:\n",
    "    f.write(str(x) + \"\\n\") # We need to convert the number into a string. We will also add an \"end-of-line\" character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close() # The file needs to be closed to guarantee all data has been written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets read this back into our program. We will first read the whole file into an array, and then convert that data into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = open(\"03_standard_way.dat\", \"r\")\n",
    "a = f_in.readlines()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the data is contained in strings. Normally we don't want this, but rather as numbers. We got some converting to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_float = []\n",
    "for x in a:\n",
    "    a_float.append(float(x)) #Convert x to float and then add to the new list\n",
    "    \n",
    "a_float = np.array(a_float) #Convert a_float into a numpy array\n",
    "print(a_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Write a cell that writes a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate a matrix using `m = np.random.rand(5,6)`\n",
    "2. Open a file called `03_standard_matrix.dat`\n",
    "3. Write a loop to write a single row of the matrix. Remember to add an empty space (\" \") between entries!\n",
    "4. Write an external loop to finish each line with an end-of-line (\"\\n\")\n",
    "5. Close the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus task**: Read the matrix back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus content:\n",
    "\n",
    "As this open-use-close structure is so common, Python has the `with` command, which simplifies a bit this loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the file usage is clearly local, and it is closed automatically! Slightly better, but still a lot of work.\n",
    "\n",
    "\n",
    "----- End of bonux content -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quite a lot of work to read the file! We can grandly simplify this by using the Numpy functions `savetxt` and `loadtxt`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Numpy way: `savetxt` and `loadtxt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has two sibling functions, called `savetxt` and `loadtxt`. The names are rather self explanatory: they save and load data to/from plain text files. \n",
    "\n",
    "Plain text has a serious advantage in relation to every other file format: They are extremely simple to read. You can expect your file to be readable in any computer ever produced, and any future produced computer. This is a great advantage, but it also has some disadvantages. First and foremost, it is relatively unstructured, carrying little metadata. Additionally, it can be a surprisingly inneficient way to store data. We will talk about more data formats later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0, 1) #Generate some data \n",
    "np.savetxt(\"03_python_way.dat\", a) #Save our data into a file\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.loadtxt(\"03_python_way.dat\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muuuch easier than before! It only takes a single command to do what took us 5 lines to do beforehand. And that scales well for other formats as well. If you want to do the same thing for a matrix, little would change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.random.rand(5, 6)\n",
    "np.savetxt(\"03_python_way_matrix.dat\", m) #Save our data into a file\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = np.loadtxt(\"03_python_way_matrix.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple datasets per file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main disadvantages of the plain-text format is the lack of data organization/metadata. When adding multiple arrays, one can organize them into columns, add a header. This is a good, even if not ideal solution. But what about the case when one has several matrices, perhaps of different sizes? \n",
    "\n",
    "In this case, we commonly use the npz format. This file has some advantages over the plain text:\n",
    "\n",
    "1. It can save the name as well as the data itself\n",
    "2. It can be compressed, taking less space\n",
    "3. It is binary, meaning no loss of precision\n",
    "\n",
    "They are mostly associated with numpy, but there are several libraries to read them in other programming languages as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.rand(100, 100)\n",
    "np.savez(\"03_numpy_way.npz\", matrix1) #Write a npz file\n",
    "print(matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"03_numpy_way.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data returned in called a dictionary. We can check which datasets we have by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that is not the original name! Unfortunately, Python cannot automatically get the name of the variables. Thus, we need to specify when saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"03_numpy_way.npz\", saved_matrix=matrix1) #Write a npz file\n",
    "data = np.load(\"03_numpy_way.npz\")\n",
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are talking! All left for now is to print the saved matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = data['saved_matrix']\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets now do this for several matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.rand(100, 100)\n",
    "matrix2 = np.random.rand(30, 52)\n",
    "matrix3 = np.random.rand(99, 128)\n",
    "matrix4 = np.random.rand(300, 1000)\n",
    "np.savez(\"03_numpy_way_multi.npz\", matrix1=matrix1, matrix2=matrix2, matrix3=matrix3, matrix4=matrix4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"03_numpy_way_multi.npz\")\n",
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gang is all here. If you are using only python, I strongly recommmend you consitently use plain files and/or npz files. However, the reality is that often another file format for data is commonly used: Excel files. They are not completely trivial to import in python, but we can do it. For this, we will use the Pandas library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read/write data from Excel, we will use the `Pandas` library. This works by leveraging the `DataFrame` structure of Pandas. You can think of this data structure as a mix between the arrays from Numpy and Excel data. The largest change is that the `DataFrames` can have named rows/columns, which make them really useful for data science. As always, we want to keep the metadata close to the data.\n",
    "\n",
    "This is a good moment to open the file `03_scaling.xlsx` and explore its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./03_scaling.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can select any given column using its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = data[\"Cores\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we actually need a subsection of the full data. For this, we can use the `loc` method. This works very similarly to the array indexing in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Select the speedup for `cores <= 4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files with more than one sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently, we aggregate several sheets into a single Excel file. By default, Pandas will read the first one of these, which is not necessarily what we want. There are several ways to handle this. \n",
    "\n",
    "First, we can tell Pandas to read the $i^{th}$ sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./03_scaling_multiple.xlsx\", sheet_name=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explictly choose the sheet by its name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./03_scaling_multiple.xlsx\", sheet_name=\"System2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, we can read the several sheets at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.read_excel(\"./03_scaling_multiple.xlsx\", sheet_name=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = datas[0]\n",
    "data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = datas[1]\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Try to do load both sheets at once, but using the name of the sheets: \"System1\" and \"System2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**: Try passing `None` to `sheet_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, `DataFrame`s work in the same way a normal Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"./03_scaling.xlsx\")\n",
    "data[\"Cores\"]+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving an Excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we manipulated the data, we can save it into a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel(\"03_scaling_extracores.xlsx\", sheet_name=\"System1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with this method we can only save a single sheet per file. We can overcome this issue with the help of `ExcelWriter`, an utility class from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read some data, just be sure we have something to work with\n",
    "# Normally, one would have manipulated the data before simply\n",
    "# rewriting\n",
    "datas = pd.read_excel(\"./03_scaling_multiple.xlsx\", sheet_name=[\"System1\", \"System2\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('03_scaling_extracores_multiple.xlsx') as writer:\n",
    "    datas[\"System1\"].to_excel(writer, sheet_name=\"CoolNewSystem1\")\n",
    "    datas[\"System2\"].to_excel(writer, sheet_name=\"NotSoCoolSystem2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
